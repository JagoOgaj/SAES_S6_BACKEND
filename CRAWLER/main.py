import os
import numpy as np
import joblib
from typing import List, Optional, Any
from pydantic import Field
from dotenv import load_dotenv
from langchain_pinecone import PineconeVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from huggingface_hub import InferenceClient
from transformers import pipeline
from langchain.chains import LLMChain
from langchain.schema.runnable import RunnableLambda

load_dotenv()

# ğŸ”¹ Chargement des clÃ©s API
os.environ["PINECONE_API_KEY"] = os.environ.get("PINECONE_API_KEY")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = os.environ.get("HUGGINGFACEHUB_API_TOKEN")

PCA_MODEL_PATH = "pca_model.pkl"

# -------------------------------
# ğŸ“Œ Classification thÃ©matique (Zero-Shot)
# -------------------------------
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

def predict_category(question: str) -> str:
    candidate_labels = ["histoire", "mathÃ©matiques", "sciences", "philosophie", "Ã©conomie", "Ã©ducation"]
    result = classifier(question, candidate_labels)
    return f"Ã©ducation, {result['labels'][0]}"

# -------------------------------
# ğŸ“Œ Expansion de requÃªte (AmÃ©lioration de la recherche)
# -------------------------------
query_expansion_model = pipeline("text2text-generation", model="facebook/bart-large-cnn")

def expand_query(question: str) -> str:
    reformulated_query = query_expansion_model(question, max_length=64, do_sample=False)
    return reformulated_query[0]["generated_text"]

# -------------------------------
# ğŸ“Œ Normalisation des embeddings (pour la cohÃ©rence)
# -------------------------------
def normalize_embeddings(embedding: np.ndarray) -> np.ndarray:
    norm = np.linalg.norm(embedding)
    return embedding / norm if norm != 0 else embedding

# -------------------------------
# ğŸ“Œ Embeddings personnalisÃ©s avec PCA si disponible
# -------------------------------
class CustomEmbeddings(HuggingFaceEmbeddings):
    pca_model_path: Optional[str] = Field(default=None)
    pca: Optional[Any] = Field(default=None, exclude=True)

    def __init__(self, model_name: str, pca_model_path: Optional[str] = None, **kwargs):
        super().__init__(model_name=model_name, **kwargs)
        object.__setattr__(self, "pca_model_path", pca_model_path)
        if pca_model_path and os.path.exists(pca_model_path):
            self.pca = joblib.load(pca_model_path)
            print(f"âœ… PCA model chargÃ© depuis {pca_model_path}.")
        else:
            print("âš ï¸ Aucun modÃ¨le PCA trouvÃ©. Utilisation de la dimension d'origine.")

    def embed_query(self, text: str) -> List[float]:
        # Obtenir l'embedding de base via le modÃ¨le
        base_embedding = super().embed_query(text)
        base_embedding = np.array(base_embedding)
        # Squeeze pour obtenir un vecteur plat (Ã©liminer les dimensions superflues)
        base_embedding = np.squeeze(base_embedding)
        # Si PCA est appliquÃ©, on transforme l'embedding (reshape nÃ©cessaire pour PCA)
        if self.pca:
            base_embedding = self.pca.transform(base_embedding.reshape(1, -1))[0]
        # Normalisation finale
        normalized = normalize_embeddings(base_embedding)
        return normalized.tolist()

# ğŸ”¹ Initialisation des embeddings
embeddings = CustomEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2", pca_model_path=PCA_MODEL_PATH)

# -------------------------------
# ğŸ“Œ Configuration de Pinecone
# -------------------------------
index_name = "education-index"
vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings, text_key='content')

retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 7, "score_threshold": 0.5, "include_metadata": True, "include_values": True}  # âœ… Ajout des options
)

# -------------------------------
# ğŸ“Œ Initialisation du modÃ¨le de langage (LLM)
# -------------------------------
hf_client = InferenceClient(model="mistralai/Mistral-7B-Instruct-v0.3", token=os.environ["HUGGINGFACEHUB_API_TOKEN"])

def call_llm(inputs):
    prompt_text = str(inputs)  # Conversion explicite en chaÃ®ne
    response = hf_client.chat_completion(
        messages=[{"role": "user", "content": prompt_text}],
        temperature=0.5,
    )
    return response["choices"][0]["message"]["content"]

llm = RunnableLambda(call_llm)  # Rend InferenceClient compatible avec LangChain

# -------------------------------
# ğŸ“Œ DÃ©finition du PromptTemplate
# -------------------------------
prompt_template = PromptTemplate(
    template="""
    Tu es un expert en {expertise}.
    RÃ©ponds prÃ©cisÃ©ment Ã  la question posÃ©e en utilisant uniquement les extraits fournis ci-dessous.
    Explique chaque Ã©vÃ©nement en dÃ©taillant ses causes et consÃ©quences immÃ©diates.
    Corrige les erreurs et reformule les rÃ©ponses de maniÃ¨re claire et exacte.
    Ne fais aucune supposition si les informations ne sont pas dans les extraits.

    Contexte :
    {context}

    Question :
    {question}

    RÃ©ponse dÃ©taillÃ©e et sourcÃ©e :
    """,
    input_variables=["expertise", "context", "question"]
)

# -------------------------------
# Utilisation de RunnableSequence pour crÃ©er la chaÃ®ne
# -------------------------------
llm_chain = prompt_template | llm

# -------------------------------
# ğŸ“Œ Fonction principale pour poser une question
# -------------------------------
def ask_question(query: str):
    expertise = predict_category(query)
    print(f"ğŸ” CatÃ©gorie dÃ©tectÃ©e : {expertise}")

    expanded_query = expand_query(query)
    docs = retriever.invoke(expanded_query)

    if not docs:
        print("âš ï¸ Aucun document trouvÃ© correspondant Ã  la requÃªte.")
        return

    context = "\n\n".join([doc.page_content for doc in docs])
    result = llm_chain.invoke({"expertise": expertise, "context": context, "question": expanded_query})

    print("\nğŸ“ RÃ©ponse dÃ©taillÃ©e :\n", result)
    print("\nğŸ“Œ Sources utilisÃ©es :")
    for doc in docs:
        metadata = doc.metadata
        title = metadata.get('title', 'Titre inconnu')
        url = metadata.get('url', 'URL inconnue')
        print(f"- **{title}** ({url})")  
        print(f"  **Extrait :** {doc.page_content[:300]}...\n")

# -------------------------------
# ğŸ“Œ Interface CLI pour poser une question
# -------------------------------
if __name__ == '__main__':
    question = input("\nğŸ” Quelle est votre question ? ")
    ask_question(question)